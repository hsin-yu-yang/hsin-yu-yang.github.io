{
  "hash": "733c532e387a00b427ed65c1d7e9dbba",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Algorithms: Linear Regression\"\nauthor: \"Ella Yang\"\ndate: \"2023-10-23\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\nIn this plot, we will apply the linear regression algorithm to predict the final grades of high school students in a math class based on various attributes. The data used for this analysis is extracted from the Student Performance dataset available at the [ UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/320/student+performance)\n\n## Importing Modules/Packages\nBefore diving into coding, we import the required modules and packages:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport pickle\n```\n:::\n\n\n## Loading in Our Data\nNext, we load the dataset and display the first 5 students using the $.head()$ function\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata =pd.read_csv(\"/Users/ella.yang/Desktop/ds499/hsin-yu-yang.github.io/data/student_mat.csv\", sep=\";\")\n#print(data.head())\n```\n:::\n\n\n## Trimming Our Data\nConsidering the relevance of attributes, we select the ones we want to use, resulting in a data frame with information related to only 6 attributes:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata = data[[\"G1\", \"G2\", \"G3\", \"studytime\", \"failures\", \"absences\"]]\n#Compare to the original data\nprint(data.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   G1  G2  G3  studytime  failures  absences\n0   5   6   6          2         0         6\n1   5   5   6          2         0         4\n2   7   8  10          2         3        10\n3  15  14  15          3         0         2\n4   6  10  10          2         0         4\n```\n:::\n:::\n\n\n## Separating Our Data\nNow that we've trimmed our data set down we need to separate it into 4 arrays. However, before we can do that we need to define what attribute we are trying to predict. This attribute is known as a label. The other attributes that will determine our label are known as features. Once we've done this we will use numpy to create two arrays. One that contains all of our features and one that contains our labels\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\npredict = \"G3\"\nX = np.array(data.drop(columns=[predict]))  # return new dataset without G3\ny = np.array(data[predict])\n```\n:::\n\n\nAfter this we need to split our data into testing and training data. We will use 90% of our data to train and the other 10% to test. The reason we do this is so that we do not test our model on data that it has already seen.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n```\n:::\n\n\nWe are ready to implement the linear regression algorithm\n\n## How Does it Work?\nLinear Regression is like drawing a line through a group of points on a graph to represent their overall trend. Imagine you have some points scattered on a graph, and you want to draw a line that best fits through them.\n\nThis line is defined by an equation: $y = m*x + b$. Here, $m$ is like the steepness of the line, telling you how much the y (up and down) value changes when x (left and right) changes. And $b$ is where the line crosses the y-axis.\n\nTo figure out the steepness ($m$) of the line, you choose two points on the line and use a formula: $m = (y2 - y1) / (x2 - x1)$. This helps in calculating how much the line slopes.\n\nOnce the computer figures out this line, it can use it to make predictions for other points.\n\nRemember, the examples we talked about are like drawing lines in two-dimensional space. In real situations, lines might go in more than just left-right and up-down directions, and that's when you get multiple slope values.\n\n## Implementing the Algorithm\nLet's use the algorithm to predict students' final grades. We'll start by defining the model we'll use\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nlinear = linear_model.LinearRegression()\n```\n:::\n\n\nAfter defining our model, we'll train and score it using the arrays we created earlier. We'll assess how well our algorithm performed on our test data by measuring R-squared\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nlinear.fit(x_train, y_train)\nacc = linear.score(x_test, y_test) #R-squared of the model\nprint(acc) #a score of above 80% is fairly good\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.852004747478786\n```\n:::\n:::\n\n\n## Viewing The Constants\nNow, let's take a look at the constants used to generate the line\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint('Coefficient: \\n', linear.coef_) # These are each slope value\nprint('Intercept: \\n', linear.intercept_) # This is the intercept\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficient: \n [ 0.1615728   0.97520329 -0.18595676 -0.22754441  0.03731939]\nIntercept: \n -1.5720906413133449\n```\n:::\n:::\n\n\n## Predicting on Specific Students\nWhile seeing a score value is interesting, let's see how well our algorithm works on specific students. We'll print out all of our test data, along with the actual final grade and our model's predicted grade\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\npredictions = linear.predict(x_test) # Gets a list of all predictions\n\n#for x in range(len(predictions)):\n    #print(predictions[x], x_test[x], y_test[x])#input data(x_test[x]), actual value for final grade (y_test[x])\n#\"G1\", \"G2\", \"G3\", \"studytime\", \"failures\", \"absences\"\n```\n:::\n\n\n![Output](o.jpeg) \n\n## Saving Our Model\nWe'll write it to a new file using $pickle.dump()$ to save our model\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nwith open(\"studentgrades.pickle\", \"wb\") as f:\n    pickle.dump(linear, f)\n\n# linear is the name of the model we created before\n# it should be defined above this\n```\n:::\n\n\n## Loading Our Model\nNow, let's remove the code that creates and trains our model as we're simply loading in an existing one from our pickle file\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\npickle_in = open(\"studentgrades.pickle\", \"rb\")\nlinear = pickle.load(pickle_in)\n\n# Now we can use linear to predict grades like before\n```\n:::\n\n\n## Training Multiple Models\nIf you've noticed that our models show different levels of R-squared, it's because each time we divide the data into training and testing sets, it's done in a unique way. Given that our model trains quite swiftly, it might be beneficial to train several models and keep the best-performing one\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# TRAIN MODEL MULTIPLE TIMES FOR BEST SCORE\nbest = 0\nfor _ in range(20):\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n\n    linear = linear_model.LinearRegression()\n\n    linear.fit(x_train, y_train)\n    acc = linear.score(x_test, y_test)\n    print(\"Accuracy: \" + str(acc))\n    \n    # If the current model has a better score than one we've already trained then save it\n    if acc > best:\n        best = acc\n        with open(\"studentgrades.pickle\", \"wb\") as f:\n            pickle.dump(linear, f)\n            # linear is the name of the model we created before\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9307728812717458\nAccuracy: 0.9051110633425279\nAccuracy: 0.8147214378563499\nAccuracy: 0.7686144104610797\nAccuracy: 0.8469484800672369\nAccuracy: 0.8760843270223411\nAccuracy: 0.8349613171153357\nAccuracy: 0.9143829866395617\nAccuracy: 0.8163677652822097\nAccuracy: 0.8598936070391182\nAccuracy: 0.854151738455897\nAccuracy: 0.8409009718487583\nAccuracy: 0.8042917177038538\nAccuracy: 0.8146505814713763\nAccuracy: 0.859206891413676\nAccuracy: 0.887279745412528\nAccuracy: 0.8016152322489802\nAccuracy: 0.8402057693065761\nAccuracy: 0.6865875166278759\nAccuracy: 0.7896708452154113\n```\n:::\n:::\n\n\n## Plotting Our Data\nTo visually understand our data, we'll create plots using the matplotlib library. We'll use a scatter plot to present our data\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Drawing and plotting model\nplot = \"absences\" # Change this to G1, G2, studytime or absences to see other graphs\nplt.scatter(data[plot], data[\"G3\"]) \nplt.legend(loc=4)\nplt.xlabel(plot)\nplt.ylabel(\"Final Grade\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=597 height=429}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}