{
  "hash": "326502f1e86902b81842ed57eef42a6c",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Algorithms: K Means Clustering\"\nauthor: \"Ella Yang\"\ndate: \"2023-12-5\"\ncategories: [news, code, analysis]\nimage: \"thumbnail.jpg\"\n---\n\nIn this post, we'll explore the concept of K-Means Clustering and walk through the process of implementing it. [Reference](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)\n\n## K-Means Clustering\nK-Means clustering is a type of unsupervised learning, which means it doesn't require labeled information for training. Unlike supervised learning where we provide the algorithm with labeled data, unsupervised learning, as in K-Means, focuses on understanding the inherent patterns and differences within the data points to identify distinct groups.\n\n## Supervised vs Unsupervised Algorithm\nTo understand the difference between supervised and unsupervised algorithms, let's recall our previous experience with supervised learning. In supervised learning, we shared both the features (attributes) of the data and their corresponding labels (Output variable, which you're trying to predict). For example, when we were classifying the higher education of students we gave the algorithm the features of the students and we told it if the students wanted to take higher education or not. Unsupervised algorithms, on the other hand, work  with the features and don't rely on predefined output variable.\n\n## How K-Means Clustering Works\nNow, let's delve into how the K-Means clustering algorithm operates. This algorithm aims to categorize data points into K unique clusters. Here's a simplified breakdown of its steps:\n\n__Step 1__ Randomly choose K points as centroids. Centroids act as representative points for each cluster\n\n__Step 2__ Assign each data point to the closest centroid based on distance\n\n__Step 3__ Calculate the average of all points within each centroid's cluster to find the center of mass. Adjust the centroids accordingly\n\n__Step 4__ Reassign each point to the closest centroid\n\n__Step 5__ Repeat steps 3-4 until no point changes its assigned centroid\n\nK-Means clustering segments our data space into different regions, each representing a distinct class. When making predictions, the algorithm identifies the section (cluster) to which a point belongs and assigns it to that particular class.\n\n## Implementing K Means Clustering \nWe will implement the K Means algorithm to classify hand written digits\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import scale\nfrom sklearn.datasets import load_digits\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n```\n:::\n\n\n## Loading the Dataset\nWe load the dataset from the sklearn module and scale down our data using the $scale()$ function. Scaling helps convert large values into a range between -1 and 1, making calculations simpler for training\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndigits = load_digits()\ndata = scale(digits.data)\ny = digits.target\n\nk = 10\nsamples, features = data.shape\n```\n:::\n\n\nWe specify the number of clusters with the variable k and determine the number of samples and features by inspecting the dataset shape.\n\n## Scoring\nTo assess our model, we use a function from the sklearn website, which computes various scores for different aspects of our model. More details about these values can be found on [website](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef bench_k_means(estimator, name, data):\n    estimator.fit(data)\n    print('%-9s\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n          % (name, estimator.inertia_,\n             metrics.homogeneity_score(y, estimator.labels_),\n             metrics.completeness_score(y, estimator.labels_),\n             metrics.v_measure_score(y, estimator.labels_),\n             metrics.adjusted_rand_score(y, estimator.labels_),\n             metrics.adjusted_mutual_info_score(y,  estimator.labels_),\n             metrics.silhouette_score(data, estimator.labels_,\n                                      metric='euclidean')))\n```\n:::\n\n\n## Training the Model\nTo train our model, we create a K Means classifier and pass it to the function we created above for scoring and training\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclf = KMeans(n_clusters=k, init=\"random\", n_init=10)\nbench_k_means(clf, \"1\", data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1        \t69689\t0.668\t0.708\t0.687\t0.554\t0.684\t0.141\n```\n:::\n:::\n\n\n## Visualization\nFor a visual representation of how K Means works, we use Principal Component Analysis (PCA) to simplify the complexity in high-dimensional data while retaining trends and patterns. We then create a K Means classifier, fit the model to the reduced data, and plot the data points. The centroids are marked with white crosses\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Reduce data dimensionality using PCA\nreduced_data = PCA(n_components=2).fit_transform(data)\n\n# Set the number of clusters (k)\nk = 10\n\n# Create a K Means classifier\nkmeans = KMeans(n_clusters=k, init=\"k-means++\", n_init=4)\n#ensures a more effective initialization of cluster centers by selecting initial centroids with a strategy that enhances convergence speed and clustering accuracy\n\n# Fit the model to the reduced data\nkmeans.fit(reduced_data)\n\n# Plot the data points\nplt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans.labels_, cmap='Paired', s=20)\n\n# Plot the centroids as white crosses\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=100, color=\"w\")\n\n# Set plot titles and labels\nplt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\nCentroids are marked with white cross\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index2_files/figure-html/cell-6-output-1.png){width=600 height=467}\n:::\n:::\n\n\n",
    "supporting": [
      "index2_files"
    ],
    "filters": [],
    "includes": {}
  }
}