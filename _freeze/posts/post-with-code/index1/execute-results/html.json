{
  "hash": "960933f273c81bbbe6fe3693783cec7a",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Algorithms: K-Nearest Neighbors\"\nauthor: \"Ella Yang\"\ndate: \"2023-11-23\"\ncategories: [news, code, analysis]\nimage: \"thumbnail.jpg\"\n---\n\nIn this plot, we will use the same datset and apply the K-Nearest Neighbors (KNN) algorithm to predict wether high school students want to take higher education based on various attributes. KNN stands for K-Nearest Neighbors where K is any kind of integer and used for classifying data.\n\n## Importing Modules/Packages\nBefore we get started, let's bring in a few modules. These will help us normalize our data and convert non-numeric values into numeric ones\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn import preprocessing, linear_model\nimport sklearn\nfrom sklearn.utils import shuffle\nfrom sklearn.neighbors import KNeighborsClassifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n```\n:::\n\n\n## Loading Data\nAfter placing our student data file into our current script directory, we can load our data. To do this, we'll use the pandas module, as seen in a previous post\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata =pd.read_csv(\"/Users/ella.yang/Desktop/ds499/hsin-yu-yang.github.io/data/student_mat.csv\", sep=\";\")\n#print(data.head())\ndata = data[[\"famsize\", \"Mjob\", \"Fjob\", \"schoolsup\", \"famsup\", \"activities\", \"higher\"]]\n```\n:::\n\n\n## Converting Data\nAs you may have noticed, many of our selected columns are not numeric. To train the K-Nearest Neighbor Classifier, we need to convert any string data into numbers. Luckily, sklearn has a method that can handle this for us.\n\nWe will start by creating a label encoder object and then use that to convert non-numeric data into numbers.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nle = preprocessing.LabelEncoder()\n```\n:::\n\n\nThe method $fit_transform()$ takes a list (each of our columns) and will turning words into numbers\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfamsize = le.fit_transform(list(data[\"famsize\"]))\nMjob = le.fit_transform(list(data[\"Mjob\"]))\nFjob = le.fit_transform(list(data[\"Fjob\"]))\nschoolsup = le.fit_transform(list(data[\"schoolsup\"]))\nfamsup = le.fit_transform(list(data[\"famsup\"]))\nactivities = le.fit_transform(list(data[\"activities\"]))\nhigher = le.fit_transform(list(data[\"higher\"]))\n```\n:::\n\n\nAfter converting, we use the $zip()$ function to group our data into two lists: one for features and another for labels, making it easier to work with\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nX = list(zip(famsize, Mjob, Fjob, schoolsup, famsup, activities))  # features\ny = list(higher)  # labels\n```\n:::\n\n\nLastly, we divide our data into two parts – one for training our model and the other for testing its performance\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n#print(x_train,y_test)\n```\n:::\n\n\n## How Does it Work?\nImagine you have points in space, and you want to know to which group a new point belongs. K-Nearest Neighbors (KNN) helps by looking at the closest points (neighbors) and deciding based on what most of those neighbors are. The \"k\" represents how many neighbors to consider.\n\nWhen \"k\" is small, KNN might focus too much on very close points, potentially making mistakes. If \"k\" is large, KNN may not pay enough attention to nearby points, leading to errors. Finding the right \"k\" is crucial.\n\n## Training a KNN Classifier\nCreating a KNN Classifier is similar to linear regression, but now we set the number of neighbors (n_neighbors)\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=9)\n```\n:::\n\n\nTraining our model involves the same steps\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmodel.fit(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=9)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nTo evaluate our model\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Making predictions on the test set\npredicted = model.predict(x_test)\n\naccuracy = accuracy_score(y_test, predicted)\nprecision = precision_score(y_test, predicted, average='weighted')\nrecall = recall_score(y_test, predicted, average='weighted')\nf1 = f1_score(y_test, predicted, average='weighted')\n\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.93\nPrecision: 0.86\nRecall: 0.93\nF1 Score: 0.89\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n```\n:::\n:::\n\n\n__Accuracy__ think of accuracy as the overall correctness of our model. It's like checking how many questions you answered correctly in a test, and expressing it as a percentage.\n\n__Precision__ tells us how good the model is at not calling something positive when it's actually negative. Imagine it as the accuracy of your positive guesses – how many of the things you said were positive are actually positive.\n\n__Recall__ is about the model's ability to find all the positive instances. It's like checking how many of the actual positive things you managed to catch among all the positive things that exist.\n\n__F1 Score__ is like finding a balance between making accurate positive guesses and catching all the positive instances. It considers both precision and recall, providing a single number to assess overall performance.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Initialize a KNN model with different values of k\nk_values = [1, 3, 5, 7, 9, 11]\naccuracy_scores = []\n\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(x_train, y_train)\n    \n    # Evaluate the model\n    predicted = model.predict(x_test)\n    accuracy = accuracy_score(y_test, predicted)\n    accuracy_scores.append(accuracy)\n\n# Plotting the results\nstyle.use('ggplot')\nplt.plot(k_values, accuracy_scores, marker='o')\nplt.title('KNN Model Accuracy for Different k Values')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Accuracy Score')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index1_files/figure-html/cell-11-output-1.png){width=600 height=454}\n:::\n:::\n\n\n## Testing Our Model\nTo see how well our model predicts, we can check its predictions against the actual outcomest\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnames = [\"unacc\", \"acc\", \"good\", \"vgood\"]\n\nfor x in range(len(predicted)):\n    #print(\"Predicted: \", names[predicted[x]], \"Data: \", x_test[x], \"Actual: \", names[y_test[x]])\n    n= model.kneighbors([x_test[x]], 9, True) #distance between the 9 neighbors\n    #print(\"N: \", n)\n\n# This will display the predicted value, our data and the actual value\n# We create a names list so that we can convert our integer predictions into their string representation \n```\n:::\n\n\n![Output](o1.jpg)\n\n__Looking at Neighbors__\n\nKNN has a method to show us the neighbors of a data point, helping us visualize our model's accuracy. We can use model.neighbors to do this\n\n![Output](o1_1.jpg) \n\n",
    "supporting": [
      "index1_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}