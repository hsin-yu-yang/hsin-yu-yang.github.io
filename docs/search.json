[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "",
    "text": "In this plot, we will apply the linear regression algorithm to predict the final grades of high school students in a math class based on various attributes. The data used for this analysis is extracted from the Student Performance dataset available at the UCI Machine Learning Repository"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hsin-yu-yang.github.io",
    "section": "",
    "text": "Machine Learning Algorithms: K Means Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nElla Yang\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning Algorithms: K-Nearest Neighbors\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nElla Yang\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning Algorithms: Linear Regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\nElla Yang\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#feature-engineering",
    "href": "posts/post-with-code/index.html#feature-engineering",
    "title": "Post With Code",
    "section": "Feature Engineering",
    "text": "Feature Engineering"
  },
  {
    "objectID": "posts/post-with-code/index.html#importing-modulespackages",
    "href": "posts/post-with-code/index.html#importing-modulespackages",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Importing Modules/Packages",
    "text": "Importing Modules/Packages\nBefore diving into coding, we import the required modules and packages:\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport pickle"
  },
  {
    "objectID": "posts/post-with-code/index.html#loading-in-our-data",
    "href": "posts/post-with-code/index.html#loading-in-our-data",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Loading in Our Data",
    "text": "Loading in Our Data\nNext, we load the dataset and display the first 5 students using the \\(.head()\\) function\n\ndata =pd.read_csv(\"/Users/ella.yang/Desktop/ds499/hsin-yu-yang.github.io/data/student_mat.csv\", sep=\";\")\n#print(data.head())"
  },
  {
    "objectID": "posts/post-with-code/index.html#trimming-our-data",
    "href": "posts/post-with-code/index.html#trimming-our-data",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Trimming Our Data",
    "text": "Trimming Our Data\nConsidering the relevance of attributes, we select the ones we want to use, resulting in a data frame with information related to only 6 attributes:\n\ndata = data[[\"G1\", \"G2\", \"G3\", \"studytime\", \"failures\", \"absences\"]]\n#Compare to the original data\nprint(data.head())\n\n   G1  G2  G3  studytime  failures  absences\n0   5   6   6          2         0         6\n1   5   5   6          2         0         4\n2   7   8  10          2         3        10\n3  15  14  15          3         0         2\n4   6  10  10          2         0         4"
  },
  {
    "objectID": "posts/post-with-code/index.html#separating-our-data",
    "href": "posts/post-with-code/index.html#separating-our-data",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Separating Our Data",
    "text": "Separating Our Data\nNow that we’ve trimmed our data set down we need to separate it into 4 arrays. However, before we can do that we need to define what attribute we are trying to predict. This attribute is known as a label. The other attributes that will determine our label are known as features. Once we’ve done this we will use numpy to create two arrays. One that contains all of our features and one that contains our labels\n\npredict = \"G3\"\nX = np.array(data.drop(columns=[predict]))  # return new dataset without G3\ny = np.array(data[predict])\n\nAfter this we need to split our data into testing and training data. We will use 90% of our data to train and the other 10% to test. The reason we do this is so that we do not test our model on data that it has already seen.\n\nx_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n\nWe are ready to implement the linear regression algorithm"
  },
  {
    "objectID": "posts/post-with-code/index.html#how-does-it-work",
    "href": "posts/post-with-code/index.html#how-does-it-work",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "How Does it Work?",
    "text": "How Does it Work?\nLinear Regression is like drawing a line through a group of points on a graph to represent their overall trend. Imagine you have some points scattered on a graph, and you want to draw a line that best fits through them.\nThis line is defined by an equation: \\(y = m*x + b\\). Here, \\(m\\) is like the steepness of the line, telling you how much the y (up and down) value changes when x (left and right) changes. And \\(b\\) is where the line crosses the y-axis.\nTo figure out the steepness (\\(m\\)) of the line, you choose two points on the line and use a formula: \\(m = (y2 - y1) / (x2 - x1)\\). This helps in calculating how much the line slopes.\nOnce the computer figures out this line, it can use it to make predictions for other points.\nRemember, the examples we talked about are like drawing lines in two-dimensional space. In real situations, lines might go in more than just left-right and up-down directions, and that’s when you get multiple slope values."
  },
  {
    "objectID": "posts/post-with-code/index.html#implementing-the-algorithm",
    "href": "posts/post-with-code/index.html#implementing-the-algorithm",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Implementing the Algorithm",
    "text": "Implementing the Algorithm\nLet’s use the algorithm to predict students’ final grades. We’ll start by defining the model we’ll use\n\nlinear = linear_model.LinearRegression()\n\nAfter defining our model, we’ll train and score it using the arrays we created earlier. We’ll assess how well our algorithm performed on our test data by measuring R-squared\n\nlinear.fit(x_train, y_train)\nacc = linear.score(x_test, y_test) #R-squared of the model\nprint(acc) #a score of above 80% is fairly good\n\n0.852004747478786"
  },
  {
    "objectID": "posts/post-with-code/index.html#viewing-the-constants",
    "href": "posts/post-with-code/index.html#viewing-the-constants",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Viewing The Constants",
    "text": "Viewing The Constants\nNow, let’s take a look at the constants used to generate the line\n\nprint('Coefficient: \\n', linear.coef_) # These are each slope value\nprint('Intercept: \\n', linear.intercept_) # This is the intercept\n\nCoefficient: \n [ 0.1615728   0.97520329 -0.18595676 -0.22754441  0.03731939]\nIntercept: \n -1.5720906413133449"
  },
  {
    "objectID": "posts/post-with-code/index.html#predicting-on-specific-students",
    "href": "posts/post-with-code/index.html#predicting-on-specific-students",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Predicting on Specific Students",
    "text": "Predicting on Specific Students\nWhile seeing a score value is interesting, let’s see how well our algorithm works on specific students. We’ll print out all of our test data, along with the actual final grade and our model’s predicted grade\n\npredictions = linear.predict(x_test) # Gets a list of all predictions\n\n#for x in range(len(predictions)):\n    #print(predictions[x], x_test[x], y_test[x])#input data(x_test[x]), actual value for final grade (y_test[x])\n#\"G1\", \"G2\", \"G3\", \"studytime\", \"failures\", \"absences\"\n\n\n\n\nOutput"
  },
  {
    "objectID": "posts/post-with-code/index.html#saving-our-model",
    "href": "posts/post-with-code/index.html#saving-our-model",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Saving Our Model",
    "text": "Saving Our Model\nWe’ll write it to a new file using \\(pickle.dump()\\) to save our model\n\nwith open(\"studentgrades.pickle\", \"wb\") as f:\n    pickle.dump(linear, f)\n\n# linear is the name of the model we created before\n# it should be defined above this"
  },
  {
    "objectID": "posts/post-with-code/index.html#loading-our-model",
    "href": "posts/post-with-code/index.html#loading-our-model",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Loading Our Model",
    "text": "Loading Our Model\nNow, let’s remove the code that creates and trains our model as we’re simply loading in an existing one from our pickle file\n\npickle_in = open(\"studentgrades.pickle\", \"rb\")\nlinear = pickle.load(pickle_in)\n\n# Now we can use linear to predict grades like before"
  },
  {
    "objectID": "posts/post-with-code/index.html#training-multiple-models",
    "href": "posts/post-with-code/index.html#training-multiple-models",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Training Multiple Models",
    "text": "Training Multiple Models\nIf you’ve noticed that our models show different levels of R-squared, it’s because each time we divide the data into training and testing sets, it’s done in a unique way. Given that our model trains quite swiftly, it might be beneficial to train several models and keep the best-performing one\n\n# TRAIN MODEL MULTIPLE TIMES FOR BEST SCORE\nbest = 0\nfor _ in range(20):\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n\n    linear = linear_model.LinearRegression()\n\n    linear.fit(x_train, y_train)\n    acc = linear.score(x_test, y_test)\n    print(\"Accuracy: \" + str(acc))\n    \n    # If the current model has a better score than one we've already trained then save it\n    if acc &gt; best:\n        best = acc\n        with open(\"studentgrades.pickle\", \"wb\") as f:\n            pickle.dump(linear, f)\n            # linear is the name of the model we created before\n\nAccuracy: 0.9307728812717458\nAccuracy: 0.9051110633425279\nAccuracy: 0.8147214378563499\nAccuracy: 0.7686144104610797\nAccuracy: 0.8469484800672369\nAccuracy: 0.8760843270223411\nAccuracy: 0.8349613171153357\nAccuracy: 0.9143829866395617\nAccuracy: 0.8163677652822097\nAccuracy: 0.8598936070391182\nAccuracy: 0.854151738455897\nAccuracy: 0.8409009718487583\nAccuracy: 0.8042917177038538\nAccuracy: 0.8146505814713763\nAccuracy: 0.859206891413676\nAccuracy: 0.887279745412528\nAccuracy: 0.8016152322489802\nAccuracy: 0.8402057693065761\nAccuracy: 0.6865875166278759\nAccuracy: 0.7896708452154113"
  },
  {
    "objectID": "posts/post-with-code/index.html#plotting-our-data",
    "href": "posts/post-with-code/index.html#plotting-our-data",
    "title": "Machine Learning Algorithms: Linear Regression",
    "section": "Plotting Our Data",
    "text": "Plotting Our Data\nTo visually understand our data, we’ll create plots using the matplotlib library. We’ll use a scatter plot to present our data\n\n# Drawing and plotting model\nplot = \"absences\" # Change this to G1, G2, studytime or absences to see other graphs\nplt.scatter(data[plot], data[\"G3\"]) \nplt.legend(loc=4)\nplt.xlabel(plot)\nplt.ylabel(\"Final Grade\")\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "posts/post-with-code/index.html#plotting-our-data-1",
    "href": "posts/post-with-code/index.html#plotting-our-data-1",
    "title": "Machine Learning Algorithms: Regression",
    "section": "Plotting Our Data",
    "text": "Plotting Our Data\nTo get a visual representation of our data we can plot it using the matplotlib library we installed earlier. We are going to use a scatter plot to visualize our data\n\n# Drawing and plotting model\nplot = \"studytime\" # Change this to G1, G2, studytime or absences to see other graphs\nplt.scatter(data[plot], data[\"G3\"]) \nplt.legend(loc=4)\nplt.xlabel(plot)\nplt.ylabel(\"Final Grade\")\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "posts/post-with-code/index1.html",
    "href": "posts/post-with-code/index1.html",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "",
    "text": "In this plot, we will use the same datset and apply the K-Nearest Neighbors (KNN) algorithm to predict wether high school students want to take higher education based on various attributes. KNN stands for K-Nearest Neighbors where K is any kind of integer and used for classifying data."
  },
  {
    "objectID": "posts/post-with-code/index1.html#importing-modulespackages",
    "href": "posts/post-with-code/index1.html#importing-modulespackages",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "Importing Modules/Packages",
    "text": "Importing Modules/Packages\nBefore we get started, let’s bring in a few modules. These will help us normalize our data and convert non-numeric values into numeric ones\n\nfrom sklearn import preprocessing, linear_model\nimport sklearn\nfrom sklearn.utils import shuffle\nfrom sklearn.neighbors import KNeighborsClassifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nfrom matplotlib import style"
  },
  {
    "objectID": "posts/post-with-code/index1.html#loading-data",
    "href": "posts/post-with-code/index1.html#loading-data",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "Loading Data",
    "text": "Loading Data\nAfter placing our student data file into our current script directory, we can load our data. To do this, we’ll use the pandas module, as seen in a previous post\n\ndata =pd.read_csv(\"/Users/ella.yang/Desktop/ds499/hsin-yu-yang.github.io/data/student_mat.csv\", sep=\";\")\n#print(data.head())\ndata = data[[\"famsize\", \"Mjob\", \"Fjob\", \"schoolsup\", \"famsup\", \"activities\", \"higher\"]]"
  },
  {
    "objectID": "posts/post-with-code/index1.html#converting-data",
    "href": "posts/post-with-code/index1.html#converting-data",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "Converting Data",
    "text": "Converting Data\nAs you may have noticed, many of our selected columns are not numeric. To train the K-Nearest Neighbor Classifier, we need to convert any string data into numbers. Luckily, sklearn has a method that can handle this for us.\nWe will start by creating a label encoder object and then use that to convert non-numeric data into numbers.\n\nle = preprocessing.LabelEncoder()\n\nThe method \\(fit_transform()\\) takes a list (each of our columns) and will turning words into numbers\n\nfamsize = le.fit_transform(list(data[\"famsize\"]))\nMjob = le.fit_transform(list(data[\"Mjob\"]))\nFjob = le.fit_transform(list(data[\"Fjob\"]))\nschoolsup = le.fit_transform(list(data[\"schoolsup\"]))\nfamsup = le.fit_transform(list(data[\"famsup\"]))\nactivities = le.fit_transform(list(data[\"activities\"]))\nhigher = le.fit_transform(list(data[\"higher\"]))\n\nAfter converting, we use the \\(zip()\\) function to group our data into two lists: one for features and another for labels, making it easier to work with\n\nX = list(zip(famsize, Mjob, Fjob, schoolsup, famsup, activities))  # features\ny = list(higher)  # labels\n\nLastly, we divide our data into two parts – one for training our model and the other for testing its performance\n\nx_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1)\n#print(x_train,y_test)"
  },
  {
    "objectID": "posts/post-with-code/index1.html#how-does-it-work",
    "href": "posts/post-with-code/index1.html#how-does-it-work",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "How Does it Work?",
    "text": "How Does it Work?\nImagine you have points in space, and you want to know to which group a new point belongs. K-Nearest Neighbors (KNN) helps by looking at the closest points (neighbors) and deciding based on what most of those neighbors are. The “k” represents how many neighbors to consider.\nWhen “k” is small, KNN might focus too much on very close points, potentially making mistakes. If “k” is large, KNN may not pay enough attention to nearby points, leading to errors. Finding the right “k” is crucial."
  },
  {
    "objectID": "posts/post-with-code/index1.html#training-a-knn-classifier",
    "href": "posts/post-with-code/index1.html#training-a-knn-classifier",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "Training a KNN Classifier",
    "text": "Training a KNN Classifier\nCreating a KNN Classifier is similar to linear regression, but now we set the number of neighbors (n_neighbors)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=9)\n\nTraining our model involves the same steps\n\nmodel.fit(x_train, y_train)\n\nKNeighborsClassifier(n_neighbors=9)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=9)\n\n\nTo evaluate our model\n\n# Making predictions on the test set\npredicted = model.predict(x_test)\n\naccuracy = accuracy_score(y_test, predicted)\nprecision = precision_score(y_test, predicted, average='weighted')\nrecall = recall_score(y_test, predicted, average='weighted')\nf1 = f1_score(y_test, predicted, average='weighted')\n\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1 Score: {f1:.2f}')\n\nAccuracy: 0.93\nPrecision: 0.86\nRecall: 0.93\nF1 Score: 0.89\n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\nAccuracy think of accuracy as the overall correctness of our model. It’s like checking how many questions you answered correctly in a test, and expressing it as a percentage.\nPrecision tells us how good the model is at not calling something positive when it’s actually negative. Imagine it as the accuracy of your positive guesses – how many of the things you said were positive are actually positive.\nRecall is about the model’s ability to find all the positive instances. It’s like checking how many of the actual positive things you managed to catch among all the positive things that exist.\nF1 Score is like finding a balance between making accurate positive guesses and catching all the positive instances. It considers both precision and recall, providing a single number to assess overall performance.\n\n# Initialize a KNN model with different values of k\nk_values = [1, 3, 5, 7, 9, 11]\naccuracy_scores = []\n\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(x_train, y_train)\n    \n    # Evaluate the model\n    predicted = model.predict(x_test)\n    accuracy = accuracy_score(y_test, predicted)\n    accuracy_scores.append(accuracy)\n\n# Plotting the results\nstyle.use('ggplot')\nplt.plot(k_values, accuracy_scores, marker='o')\nplt.title('KNN Model Accuracy for Different k Values')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Accuracy Score')\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index1.html#testing-our-model",
    "href": "posts/post-with-code/index1.html#testing-our-model",
    "title": "Machine Learning Algorithms: K-Nearest Neighbors",
    "section": "Testing Our Model",
    "text": "Testing Our Model\nTo see how well our model predicts, we can check its predictions against the actual outcomest\n\nnames = [\"unacc\", \"acc\", \"good\", \"vgood\"]\n\nfor x in range(len(predicted)):\n    #print(\"Predicted: \", names[predicted[x]], \"Data: \", x_test[x], \"Actual: \", names[y_test[x]])\n    n= model.kneighbors([x_test[x]], 9, True) #distance between the 9 neighbors\n    #print(\"N: \", n)\n\n# This will display the predicted value, our data and the actual value\n# We create a names list so that we can convert our integer predictions into their string representation \n\n\n\n\nOutput\n\n\nLooking at Neighbors\nKNN has a method to show us the neighbors of a data point, helping us visualize our model’s accuracy. We can use model.neighbors to do this\n\n\n\nOutput"
  },
  {
    "objectID": "posts/post-with-code/index2.html",
    "href": "posts/post-with-code/index2.html",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "",
    "text": "In this post, we’ll explore the concept of K-Means Clustering and walk through the process of implementing it. Reference"
  },
  {
    "objectID": "posts/post-with-code/index2.html#k-means-clustering",
    "href": "posts/post-with-code/index2.html#k-means-clustering",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means clustering is a type of unsupervised learning, which means it doesn’t require labeled information for training. Unlike supervised learning where we provide the algorithm with labeled data, unsupervised learning, as in K-Means, focuses on understanding the inherent patterns and differences within the data points to identify distinct groups."
  },
  {
    "objectID": "posts/post-with-code/index2.html#supervised-vs-unsupervised-algorithm",
    "href": "posts/post-with-code/index2.html#supervised-vs-unsupervised-algorithm",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "Supervised vs Unsupervised Algorithm",
    "text": "Supervised vs Unsupervised Algorithm\nTo understand the difference between supervised and unsupervised algorithms, let’s recall our previous experience with supervised learning. In supervised learning, we shared both the features (attributes) of the data and their corresponding labels (Output variable, which you’re trying to predict). For example, when we were classifying the higher education of students we gave the algorithm the features of the students and we told it if the students wanted to take higher education or not. Unsupervised algorithms, on the other hand, work with the features and don’t rely on predefined output variable."
  },
  {
    "objectID": "posts/post-with-code/index2.html#how-k-means-clustering-works",
    "href": "posts/post-with-code/index2.html#how-k-means-clustering-works",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "How K-Means Clustering Works",
    "text": "How K-Means Clustering Works\nNow, let’s delve into how the K-Means clustering algorithm operates. This algorithm aims to categorize data points into K unique clusters. Here’s a simplified breakdown of its steps:\nStep 1 Randomly choose K points as centroids. Centroids act as representative points for each cluster\nStep 2 Assign each data point to the closest centroid based on distance\nStep 3 Calculate the average of all points within each centroid’s cluster to find the center of mass. Adjust the centroids accordingly\nStep 4 Reassign each point to the closest centroid\nStep 5 Repeat steps 3-4 until no point changes its assigned centroid\nK-Means clustering segments our data space into different regions, each representing a distinct class. When making predictions, the algorithm identifies the section (cluster) to which a point belongs and assigns it to that particular class."
  },
  {
    "objectID": "posts/post-with-code/index2.html#implementing-k-means-clustering",
    "href": "posts/post-with-code/index2.html#implementing-k-means-clustering",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "Implementing K Means Clustering",
    "text": "Implementing K Means Clustering\nWe will implement the K Means algorithm to classify hand written digits\n\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import scale\nfrom sklearn.datasets import load_digits\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA"
  },
  {
    "objectID": "posts/post-with-code/index2.html#loading-the-dataset",
    "href": "posts/post-with-code/index2.html#loading-the-dataset",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "Loading the Dataset",
    "text": "Loading the Dataset\nWe load the dataset from the sklearn module and scale down our data using the \\(scale()\\) function. Scaling helps convert large values into a range between -1 and 1, making calculations simpler for training\n\ndigits = load_digits()\ndata = scale(digits.data)\ny = digits.target\n\nk = 10\nsamples, features = data.shape\n\nWe specify the number of clusters with the variable k and determine the number of samples and features by inspecting the dataset shape."
  },
  {
    "objectID": "posts/post-with-code/index2.html#scoring",
    "href": "posts/post-with-code/index2.html#scoring",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "Scoring",
    "text": "Scoring\nTo assess our model, we use a function from the sklearn website, which computes various scores for different aspects of our model. More details about these values can be found on website\n\ndef bench_k_means(estimator, name, data):\n    estimator.fit(data)\n    print('%-9s\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n          % (name, estimator.inertia_,\n             metrics.homogeneity_score(y, estimator.labels_),\n             metrics.completeness_score(y, estimator.labels_),\n             metrics.v_measure_score(y, estimator.labels_),\n             metrics.adjusted_rand_score(y, estimator.labels_),\n             metrics.adjusted_mutual_info_score(y,  estimator.labels_),\n             metrics.silhouette_score(data, estimator.labels_,\n                                      metric='euclidean')))"
  },
  {
    "objectID": "posts/post-with-code/index2.html#training-the-model",
    "href": "posts/post-with-code/index2.html#training-the-model",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "Training the Model",
    "text": "Training the Model\nTo train our model, we create a K Means classifier and pass it to the function we created above for scoring and training\n\nclf = KMeans(n_clusters=k, init=\"random\", n_init=10)\nbench_k_means(clf, \"1\", data)\n\n1           69689   0.668   0.708   0.687   0.554   0.684   0.141"
  },
  {
    "objectID": "posts/post-with-code/index2.html#visualization",
    "href": "posts/post-with-code/index2.html#visualization",
    "title": "Machine Learning Algorithms: K Means Clustering",
    "section": "Visualization",
    "text": "Visualization\nFor a visual representation of how K Means works, we use Principal Component Analysis (PCA) to simplify the complexity in high-dimensional data while retaining trends and patterns. We then create a K Means classifier, fit the model to the reduced data, and plot the data points. The centroids are marked with white crosses\n\n# Reduce data dimensionality using PCA\nreduced_data = PCA(n_components=2).fit_transform(data)\n\n# Set the number of clusters (k)\nk = 10\n\n# Create a K Means classifier\nkmeans = KMeans(n_clusters=k, init=\"k-means++\", n_init=4)\n#ensures a more effective initialization of cluster centers by selecting initial centroids with a strategy that enhances convergence speed and clustering accuracy\n\n# Fit the model to the reduced data\nkmeans.fit(reduced_data)\n\n# Plot the data points\nplt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans.labels_, cmap='Paired', s=20)\n\n# Plot the centroids as white crosses\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=100, color=\"w\")\n\n# Set plot titles and labels\nplt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\nCentroids are marked with white cross\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\n\n# Show the plot\nplt.show()"
  }
]